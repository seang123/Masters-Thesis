---
 # Name the current run
 run: no_attn_loss_const_lr2

 info: | 
     constant learning rate of 0.0001
     dropout
     deep output non linear layer
     using learning rate schedule
     trained on subj 2 samples
     standard lstm
     no adap grad clip

 # Data stores
 dataset:
     betas_path: "/fast/seagie/data/subj_2/betas_averaged/"
     captions_path: "/fast/seagie/data/subj_2/captions/"
     vgg16_path: "/fast/seagie/data/subj_2/vgg16/"
     nsd_dir: "/home/seagie/NSD2"
 log: "./Log/"

 seed: 42

 # Training
 epochs: 200 # 25
 batch_size: 64
 max_length: 15
 optimizer: Adam
 alpha: 0.0001
 clipnorm: 0.1
 decay: 0 #1.0e-4

 dropout_input: 0 # 0.1 
 dropout_features:  0.2 # 0.2 
 dropout_text: 0.2 # 0.2
 dropout_lstm: 0.2 # 0.2 
 dropout_attn: 0.2 # 0.2 
 dropout_out: 0.2

 input_reg: 0.01 # 0.002         # scientific notation requires decimal notation - x.0e
 attn_reg: 0.001 # 0.01
 lstm_reg: 0.00003 # 0.0003 
 output_reg: 0.00001 # 1.3e-5

 # Input size
 input: 
     full: 327684
     vc: 62756
     pca: 5000
     mscoco: 4096

 # Model size 
 units: 512 # lstm
 attn_units: 32
 group_size: 32 # acts as embedding dim for attention model
 embedding_features: 512
 embedding_text: 512
 top_k: 5000





