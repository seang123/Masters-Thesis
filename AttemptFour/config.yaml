---
 # Name the current run
 run: test

 info: | 
     using low_lr_bn3 settings, but with an additional participant 
     attention_baseline
     added batch normalization to the attention mechanism 
     added batch normalization after lc layers and before leakyrelu
     batch norm after activation function
     lowered learning rate by 10x based on analysis of 2 sample analysis 
     model got stuck in a local minima with lr=.001

 # Data stores
 dataset:
     betas_path: "/fast/seagie/data/subj_2/betas_averaged/"
     captions_path: "/fast/seagie/data/subj_2/captions/"
     vgg16_path: "/fast/seagie/data/subj_2/vgg16/"
     nsd_dir: "/home/seagie/NSD2"
 log: "./Log/"

 seed: 42

 # Training
 epochs: 100
 batch_size: 64
 max_length: 13
 optimizer: Adam
 alpha: 0.0001
 clipnorm: 1
 decay: 0 #1.0e-4

 dropout_input: 0 # 0.17
 dropout_features: 0 # 0.18
 dropout_text: 0 # 0.01
 dropout_lstm: 0
 dropout_attn: 0

 input_reg: 0.01 # 0.002         # scientific notation requires decimal notation - x.0e
 attn_reg: 0.01
 lstm_reg: 0.00003 # 0.0003 
 output_reg: 0.00001 # 1.3e-5

 # Input size
 input: 
     full: 327684
     vc: 62756
     pca: 5000
     mscoco: 4096

 # Model size 
 units: 512 # lstm
 attn_units: 32
 group_size: 32
 embedding_features: 512
 embedding_text: 512
 top_k: 5000





