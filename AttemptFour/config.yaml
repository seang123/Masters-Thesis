---
 # Name the current run
 run: attention_loss

 info: | 
     removed the relu on the attention layers
     attention loss target is 1
     using layer norm lstm
     adaptive gradient clipping
     subject 2

 # Data stores
 dataset:
     betas_path: "/fast/seagie/data/subj_2/betas_averaged/"
     captions_path: "/fast/seagie/data/subj_2/captions/"
     vgg16_path: "/fast/seagie/data/subj_2/vgg16/"
     nsd_dir: "/home/seagie/NSD2"
 log: "./Log/"

 seed: 42

 # Training
 epochs: 25
 batch_size: 64
 max_length: 13
 optimizer: Adam
 alpha: 0.0001
 clipnorm: 0.1
 decay: 0 #1.0e-4

 dropout_input: 0 # 0.1 #0.1 # 0.17
 dropout_features: 0 # 0.2 #0.33 # 0.18
 dropout_text: 0 #0.2 # 0.25 # 0.01
 dropout_lstm: 0 # 0.2 # 0.5
 dropout_attn: 0 # 0.2 #0.25

 input_reg: 0.01 # 0.002         # scientific notation requires decimal notation - x.0e
 attn_reg: 0.01
 lstm_reg: 0.00003 # 0.0003 
 output_reg: 0.00001 # 1.3e-5

 # Input size
 input: 
     full: 327684
     vc: 62756
     pca: 5000
     mscoco: 4096

 # Model size 
 units: 512 # lstm
 attn_units: 32
 group_size: 32
 embedding_features: 512
 embedding_text: 512
 top_k: 5000





